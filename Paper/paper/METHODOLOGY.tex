\section{Our Method}
\label{sec:Meth}


\subsection{System Overview}
\label{subsection:overview}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=3.4in]{figure/pipeline.eps}
	\caption{An overview of our layout estimation algorithm pipeline. First we adopt a multi-scale CNN achitecture to predict geometric information from RGB image, including depth and normals. Then we encode abovementioned information into FCNN , which help to accurately estimate the layout. Optimization framwork based on perspective projection restriction is adopted to generate final precise layout estimates.}
	\label{fig:pipeline}
\end{figure}

Under the Manhattan world assumption, a room layout is represented as cube having at most five walls (Left, Front, Right, Ceiling, Ground) visible in the image. 
%
Given an RGB image $I$ with \cxj{arbitrary?} \drf{yes, but it will be resized to $320\times 240$ and then input into the network~\cite{eigen2015predicting}}size $w\times h$, our algorithm generates a room layout $\vb{L}$ consisting of a surface label for each pixel $L_{ij}\in $ $\{$left wall, front wall, right wall, ceiling, ground$\}$. 
Fig. \ref{fig:pipeline} shows our algorithm pipeline. 
%%step 1
Different from \cite{dasgupta2016delay}, we first estimate the depth depth $D_{I}$ and normal map $N_{I}$ from the input color image to generate \emph{geometric hints} using a multi-scale convolutional architecture~\cite{eigen2015predicting}, as described in Sec.~\ref{sec:depth_normal}.
%step 2
Integrating the original RGB image, the estimated depth and the normal map, a fully convolutional network is used to predict five surface maps, each of which describes the belief for each specific layout surface. Details will be described in Sec.~\ref{sec:surfacelabel}.
%step 3: optimization
To generate more clear and straight boundaries in the final layout, an optimization step is adopted to \cxj{what does this optimzation do?}\drf{The output of the FCN-MC maybe not consistent with the model that we use to parameterize the layout. For example, the boundaries are not straight, there may be multiple disjoint components per label, and it may contain spurious regions like spurious front wall. The optimization do some preprocessing first to prune the extra disjoint components and fill in the hole caused by pruning and judge whether the conbination of the plane is reasonable. Then apply an iterative refinement process to obtain L. So the ultimate goal of the optimization step is to get L which parameterize the straight boundaries of layout.}, as described in Sec.~\ref{sec:optimization}.


\cxj{several questions here }

\begin{enumerate}
	\item \textbf{input image size}: as claimed in \cite{ren2016three}, the receptive field of VGG16 is 404x404, why do we use $321\times 321$?\ \\ \drf{Need to do experiment. As has been revealed by \cite{ren2016three}, if
		the input image size is smaller than the receptive field size, it is padded with zeros and spatial resolution is lost in this case. So $321\times 321$ may be not appropriate, we should change it to $404\times 404$}

\end{enumerate}
 

\comments{
The coarse layout estimation about semantic layout surfaces for a single RGB image, are predicted by fully convolutional neural network with geometric information emmbeded, including depth and normal information. These geometric information are estimated from source RGB image by a multi-scale convolutional architecture\cite{eigen2015predicting}. This will be decribied in Sec. \ref{subsection:CNN}. Then based on optimization framwork proposed in \cite{dasgupta2016delay}, which mainly uses perspective projection constraints, we can obtain final precise layout estimation results.
%
}
\cxj{Figure 3 is very similar with \cite{dasgupta2016delay}..}
 


\subsection{Geometric Fusion FCNN for Coarse Layout Estimation}
\label{sec:depth_normal}

We use the multi-scale convolutional network proposed in \cite{eigen2015predicting} to estimate the depth and normal map from a single RGB image, as Figure~\ref{fig:depthandnormal} shows. 
%
\cxj{More analysis on the results of depth map and normal map.}
\drf{As claimed in \cite{eigen2015predicting}, they acheive state-of-the-art performance on these two tasks. The sqr relative difference of the estimated depth maps is as low as 0.121 and the mean angle distance of the estimated normal maps is 22.2. They also evaluate their method using several other measurements for both depth and normals, see details in \cite{eigen2015predicting}. Despite the great progress they have made, the depth maps and surface maps estimated from single RGB images are still not very precise in local details. Furthermore, the The output resolution of the depth maps and normal maps are limited to half the input($147\times109$ in our case).   }


\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figure/geometricinfo1.png}
	\caption{Estimation of depth and normal from a single RGB image using the multi-task FCN in \cite{eigen2015predicting}.}
	\label{fig:depthandnormal}
\end{figure}

\mdf{Though the predicted depth and normal are not accurate}, they provide valuable 3D information for high-level structure estimation, especially in cluttered scenes.\drf{And we are more concerned about global spatial relations rather than local details.}
%
Normals can serve as clues tending to merge big planes together, with interference factors like clutter, textures and illumination eliminated to varying degrees. 
%
These mid-level geometric information can be used to adapt and improve performance for layout estimation compared to using RGB only. 



\subsection{Surface Label Prediction using MFCN}
\label{sec:surfacelabel}
%
In previous work, \cite{dasgupta2016delay,ren2016coarse} achieved to use fully convolutional neural network(FCNN) or multi-task fully convolutional neural network~(MFCNN) to predict coarse semantic layout surfaces and layout edges. 
%
However, due to much clutter, complex textures and illumination variations existed, semantic surfaces like walls are visually separated in to pieces, making whole surfaces difficult to aggregate together. 
%
Fig.~\ref{fig:fcn-comparison}(b) shows layout estimation results using FCNN architecture for prediction using a general FCN widely used in previous methods \cite{dasgupta2016delay,ren2016coarse}. 
Under the comparison between the predicted results and the ground truth, we can see that due to the environmental effect of indoor scenes, layout results estimated from FCNN are not reliable. 
When there exits clutter right on the boundaries, such critical clues are partially or entirely excluded, and thus we can not tell location of each plane precisely. 
Moreover, an entire plane may be predicted separately into pieces due to clutter lay in the plane.

\textbf{Network Architecture}
\cxj{Explain the network architecture.}
\drf{We use the network architechure proposed by \cite{ren2016coarse} but with only one output branch while \cite{ren2016coarse} is a multi-task FCN and learn both layout and semantic surface. The architechure is based on VGG16, in Fig.~\ref{fig:coarsetofine} We remove the first branch after fc7 and in turn we don't have to use a joint loss. The network is trained specifically for semantic surface segmentation. And we fuse the depth map and the normal map with rgb image as additional geometric information in the network to improve the performance of segmentation.   }

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figure/coarse-to-fine-net.png}
	\drf{\caption{The network architecture of \cite{ren2016coarse}, illustration of the FCN-VGG16 with two output branches. We only use the second branch to learn semantic surface and prune the first branch. By the way, the number of convolution layers in this figure may be wrong, i check the prototxt of VGG16 and FCN-32s, there should be three layers in the part which are pointed by black arrow. }}
	\label{fig:coarsetofine}
\end{figure}

Given the input RGB image and the estimated geometric information including depth and normal, there are several possible ways to integrate them in a network to predict the surface labels. We introduce two architectures here.
We also resize them to $321 \time 321$ as input to the following FCN. 

The first way is treading the depth and normal map as four additional channels associated with the input RGB image. Given the seven-channel input (3 channels from RGB, 1 channel from depths and 3 channels from normals), we train a \mdf{VGG16 FCN} to predict the five belief maps for the five surfaces in a room. The network architecture is shown in Fig.~\ref{fig:fcn-multi-channel}. This architecture is named as FCN-MC in this paper. 
%
However, it is intuitively seen that different channels have a wide range of variance and they probably can not be directly fused at the beginning convolutional layers. 

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figure/fcn-multi-channel.png}
	\caption{A simple network architecture taking the RGB, depth and normal together as input to a VGG16 FCN. \cxj{add more details of each layer.}}
	\label{fig:fcn-multi-channel}
\end{figure}

The second way to integrate them is to later fuse the features that are extracted from different channels separately with a number of convolutional layers. 
This network is named as FCN-GF (geometric fusion), whose architecture is shown in Fig.~\ref{fig:fcn-geometric-fusion}.
  
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figure/fcn-geometric-fusion.png}
	\caption{A network architecture that fuses the RGB, depth and normal together later. \cxj{add more details of each layer.}}
	\label{fig:fcn-geometric-fusion}
\end{figure}


For both networks to predict the five surface maps, the loss function is defined as the softmax ...\cxj{check wiht Fengjuntao. thesis page 18.}
\drf{According to fengjuntao, the loss function was written by himself but probably not standard. I check the softmaxwithloss function on the internet and i think the formulation should be written as...or ...   }
\cxj{The output is five maps or a single map with five surface labels?}
\drf{The output of the FCN-MC/FCN-GF is a $w\times h \times 5$ multidimensional array T, where w and h is the width and length of the input rgb image, and each of the 5 slices can be interpreted as a classification map for a specific label. \\A single map with five surface labels can be obtained by simply picking the label with the highest score for each pixel among those 5 slices. and we use this single map for step3: optimization. }


\textbf{Training}
\cxj{How do you train this network with additional input? Any option to change the network? Do you modified the network parameters? say number of neurons or layers?}
\drf{For FCN-MC, depth map and normals map are merged with the rgb image as new channels and input to the FCN. }

\begin{figure}[!ht]
	\centering 
	\textsc{\includegraphics[width=3.4in]{figure/fcn.eps}}
	\caption{Layout estimation results using different architectures. Row from top to bottom: (a) the input RGB image. (b)(c)(d)Surface predictions using the FCNN architecture used in \cite{dasgupta2016delay,ren2016coarse}, our FCN-MC and FCN-GF respectively. (e) The ground truth. }
	\label{fig:fcn-comparison}
\end{figure}

\cxj{Result analysis for different architures. } 
\drf{For FCN-MC, We treat the normal and the depth as homogeneous information of the rgb image and are a supplement to the rgb image. These addtitional information can serve as context constraints to improve the performance of the network.\\For FCN-GF, we treat the normal and the depth as unhomogeneous information of the rgb image and may disrupt the network parameters as they share weights with the rgb image in the following convolution layers during training. So we learn these information sepertately and fuse them in high-level layer. ie. conv5-3. (more detailed result analysis coming soon..)  }
From Table~\ref{table:fcn-accuracy}, we can see that ...\cxj{FCN-GF is better than FCN-MC?}
\drf{We expect that the performance of FCN-GF should be better than FCN-MC, but it turns out that FCN-MC slightly outperforms FCN-GF. Furthermore, the structrue of FCN-MC have less paramters than FCN-GF, so it's time saving and memory saving compared to FCN-MC.\\Although the result reveals FCN-MC is better, the structure of FCN-GF is still worth trying, as the depth maps used as input to the network are not standard. ie. They are encoded to 3 channels rgb images and should be decoded to gray scale image. And the batch size in training stage may be not appropriate due to hardware limitation. }
Our method generate much more clear edges, less holes. 


\begin{table}
	\centering
	\caption{Pixelwise accuracy for surface label prediction.}
	\label{table:fcn-accuracy}
	\begin{tabular}{c|c}
		\hline
	Network & Accuracy\\
	\hline
	FCN-32s & 0.8109 \\ 
	FCN-MC  & 0.8392 \\
	FCN-GF  & 0.8350 \\
		\hline
	\end{tabular}
		
\end{table}



\cxj{Do test on RGBD images...}




\subsection{Layout Generation}
\label{subsection:optimization}
We adopt a popular model that several  researchers~\cite{wang2013discriminative,dasgupta2016delay,ren2016coarse} have been used to parameterize indoor layout based on the Manhattan world assumption. 
Indoor scene layout is modeled as the projection of a cuboid which can be defined by 

\begin{equation}
	\label{eq:Layout}
	L = (l1, l2, l3, l4, v)
\end{equation}
where $l_{i}$ stands for $i^{th}$ vanishing line and $v$ stands for the specific vanishing point. The whole scene is equivalent to be labeled with five semantic surfaces, coresponding to (front, left, right, ceiling, ground), as described in Fig. \ref{fig:2.model}. Based on Eq. (\ref{eq:Layout}), each surface can be reconstructed with extension lines between vanishing point $v$ and Intersection point $p_i$ among $l_{i}$. ie $ve_i$ . Due to the uncertainty of camera pose, five surfaces are not always visible. Different examples are given in Fig. \ref{fig:different-layout-type}, but such layout can still be modeled by Eq. (\ref{eq:Layout}).

\textbf{Preprocessing} 
Although we have more robust surface prediction result from our FCN-MC. Problems like spurious regions and multiple disjoint components are still a bit disturbing. To address these problems we simplify the preprocessing step in ~\cite{dasgupta2016delay} for efficiency. First, we address the multiple disjoint components (which means the connection domain for each label is not unique) by remove all but the largest connected domain for each label. Then we fill the holes created by this pruning with labels around them by using k-nn. After that we apply some simple contraints based on common sense to handle spurious regions.
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figure/different-layout-type.png}
	\caption{Different layout type due to variation in camera pose. \drf{I'm not sure whether this situation should be discussed, so the caption is brief.} }
	\label{fig:different-layout-type}
\end{figure}